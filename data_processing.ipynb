{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - Considering Bias in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import statements\n",
    "import pandas as pd\n",
    "import json, time, urllib.parse\n",
    "import requests\n",
    "\n",
    "import aiohttp\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the data\n",
    "us_cities_by_state = pd.read_csv('data/us_cities_by_state_SEPT.2023.csv')\n",
    "#state_populations = pd.read_excel('data/NST-EST2022-POP.xlsx')\n",
    "#region_reference = pd.read_excel(\"data/US States by Region - US Census Bureau.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Getting page info data for: {ARTICLE_TITLES[3]}\")\n",
    "# info = request_pageinfo_per_article(ARTICLE_TITLES[3])\n",
    "# print(json.dumps(info,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inconsistencies with the state list\n",
    "\n",
    "# Dropping duplicates\n",
    "us_cities_by_state.drop_duplicates(inplace=True, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>2020 United States census</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2020_United_Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>2010 United States census</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2010_United_Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>Florida</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12947</th>\n",
       "      <td>New_York</td>\n",
       "      <td>Population</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18433</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18780</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Population</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18781</th>\n",
       "      <td>Texas</td>\n",
       "      <td>2020 United States census</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2020_United_Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18782</th>\n",
       "      <td>Texas</td>\n",
       "      <td>2010 United States census</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2010_United_Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21234</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state                 page_title  \\\n",
       "1683    Colorado  2020 United States census   \n",
       "1684    Colorado  2010 United States census   \n",
       "2030     Florida     County (United States)   \n",
       "5196        Iowa     County (United States)   \n",
       "12947   New_York                 Population   \n",
       "18433  Tennessee     County (United States)   \n",
       "18780      Texas                 Population   \n",
       "18781      Texas  2020 United States census   \n",
       "18782      Texas  2010 United States census   \n",
       "21234  Wisconsin     County (United States)   \n",
       "\n",
       "                                                     url  \n",
       "1683   https://en.wikipedia.org/wiki/2020_United_Stat...  \n",
       "1684   https://en.wikipedia.org/wiki/2010_United_Stat...  \n",
       "2030   https://en.wikipedia.org/wiki/County_(United_S...  \n",
       "5196   https://en.wikipedia.org/wiki/County_(United_S...  \n",
       "12947           https://en.wikipedia.org/wiki/Population  \n",
       "18433  https://en.wikipedia.org/wiki/County_(United_S...  \n",
       "18780           https://en.wikipedia.org/wiki/Population  \n",
       "18781  https://en.wikipedia.org/wiki/2020_United_Stat...  \n",
       "18782  https://en.wikipedia.org/wiki/2010_United_Stat...  \n",
       "21234  https://en.wikipedia.org/wiki/County_(United_S...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if we missed any unwanted entries\n",
    "a = list(us_cities_by_state.page_title)\n",
    "b = {}\n",
    "c = set()\n",
    "\n",
    "for state in a:\n",
    "    if state in b:\n",
    "        c.add(state)\n",
    "    else:\n",
    "        b[state] = 1\n",
    "us_cities_by_state[us_cities_by_state['page_title'].isin(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to remove these rows from the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_by_state.drop(us_cities_by_state[us_cities_by_state['page_title'].isin(c)].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21520</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wamsutter,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21521</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wheatland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21522</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Worland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21523</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wright,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21524</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Yoder,_Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21515 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state           page_title  \\\n",
       "0      Alabama   Abbeville, Alabama   \n",
       "1      Alabama  Adamsville, Alabama   \n",
       "2      Alabama     Addison, Alabama   \n",
       "3      Alabama       Akron, Alabama   \n",
       "4      Alabama   Alabaster, Alabama   \n",
       "...        ...                  ...   \n",
       "21520  Wyoming   Wamsutter, Wyoming   \n",
       "21521  Wyoming   Wheatland, Wyoming   \n",
       "21522  Wyoming     Worland, Wyoming   \n",
       "21523  Wyoming      Wright, Wyoming   \n",
       "21524  Wyoming       Yoder, Wyoming   \n",
       "\n",
       "                                                     url  \n",
       "0       https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "1      https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "2         https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "3           https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "4       https://en.wikipedia.org/wiki/Alabaster,_Alabama  \n",
       "...                                                  ...  \n",
       "21520   https://en.wikipedia.org/wiki/Wamsutter,_Wyoming  \n",
       "21521   https://en.wikipedia.org/wiki/Wheatland,_Wyoming  \n",
       "21522     https://en.wikipedia.org/wiki/Worland,_Wyoming  \n",
       "21523      https://en.wikipedia.org/wiki/Wright,_Wyoming  \n",
       "21524       https://en.wikipedia.org/wiki/Yoder,_Wyoming  \n",
       "\n",
       "[21515 rows x 3 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_by_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Getting last revision id for an article\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Define constants\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_THROTTLE_WAIT = 0.1\n",
    "\n",
    "# Define other constants and variables\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<aadi2000@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",  # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "       \n",
    "\n",
    "async def request_pageinfo_per_article(session, article_title, endpoint_url=API_ENWIKIPEDIA_ENDPOINT,\n",
    "                                       request_template=PAGEINFO_PARAMS_TEMPLATE, headers=REQUEST_HEADERS):\n",
    "    request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    async with session.get(endpoint_url, headers=headers, params=request_template) as response:\n",
    "        try:\n",
    "            json_response = await response.json()\n",
    "            # Get the page ID for the article\n",
    "            page_id = list(json_response['query']['pages'].keys())[0]\n",
    "            lastrevid = json_response['query']['pages'][page_id]['lastrevid']\n",
    "            return lastrevid\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to retrieve data for {article_title}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "async def main(article_titles, page_info_dict, unable_to_get):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        count = 0  # Initialize a counter\n",
    "        for i, title in enumerate(article_titles):\n",
    "            task = request_pageinfo_per_article(session, title)\n",
    "            tasks.append(task)\n",
    "            if i % 10 == 0:\n",
    "                print(f\"{i} of {len(article_titles)} have been requested\")\n",
    "                \n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"{i + 1} articles' IDs have been stored\")\n",
    "                count += 1\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        for i, response in enumerate(responses):\n",
    "            if response is not None:\n",
    "                page_info_dict[article_titles[i]] = response\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for {article_titles[i]}\")\n",
    "                unable_to_get[article_titles[i]] = 1\n",
    "        print(f\"{count * 1000} articles are done\")\n",
    "    return page_info_dict, unable_to_get\n",
    "\n",
    "# Define the list of articles\n",
    "article_list = list(us_cities_by_state[\"page_title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 1000 have been requested\n",
      "10 of 1000 have been requested\n",
      "20 of 1000 have been requested\n",
      "30 of 1000 have been requested\n",
      "40 of 1000 have been requested\n",
      "50 of 1000 have been requested\n",
      "60 of 1000 have been requested\n",
      "70 of 1000 have been requested\n",
      "80 of 1000 have been requested\n",
      "90 of 1000 have been requested\n",
      "100 of 1000 have been requested\n",
      "110 of 1000 have been requested\n",
      "120 of 1000 have been requested\n",
      "130 of 1000 have been requested\n",
      "140 of 1000 have been requested\n",
      "150 of 1000 have been requested\n",
      "160 of 1000 have been requested\n",
      "170 of 1000 have been requested\n",
      "180 of 1000 have been requested\n",
      "190 of 1000 have been requested\n",
      "200 of 1000 have been requested\n",
      "210 of 1000 have been requested\n",
      "220 of 1000 have been requested\n",
      "230 of 1000 have been requested\n",
      "240 of 1000 have been requested\n",
      "250 of 1000 have been requested\n",
      "260 of 1000 have been requested\n",
      "270 of 1000 have been requested\n",
      "280 of 1000 have been requested\n",
      "290 of 1000 have been requested\n",
      "300 of 1000 have been requested\n",
      "310 of 1000 have been requested\n",
      "320 of 1000 have been requested\n",
      "330 of 1000 have been requested\n",
      "340 of 1000 have been requested\n",
      "350 of 1000 have been requested\n",
      "360 of 1000 have been requested\n",
      "370 of 1000 have been requested\n",
      "380 of 1000 have been requested\n",
      "390 of 1000 have been requested\n",
      "400 of 1000 have been requested\n",
      "410 of 1000 have been requested\n",
      "420 of 1000 have been requested\n",
      "430 of 1000 have been requested\n",
      "440 of 1000 have been requested\n",
      "450 of 1000 have been requested\n",
      "460 of 1000 have been requested\n",
      "470 of 1000 have been requested\n",
      "480 of 1000 have been requested\n",
      "490 of 1000 have been requested\n",
      "500 of 1000 have been requested\n",
      "510 of 1000 have been requested\n",
      "520 of 1000 have been requested\n",
      "530 of 1000 have been requested\n",
      "540 of 1000 have been requested\n",
      "550 of 1000 have been requested\n",
      "560 of 1000 have been requested\n",
      "570 of 1000 have been requested\n",
      "580 of 1000 have been requested\n",
      "590 of 1000 have been requested\n",
      "600 of 1000 have been requested\n",
      "610 of 1000 have been requested\n",
      "620 of 1000 have been requested\n",
      "630 of 1000 have been requested\n",
      "640 of 1000 have been requested\n",
      "650 of 1000 have been requested\n",
      "660 of 1000 have been requested\n",
      "670 of 1000 have been requested\n",
      "680 of 1000 have been requested\n",
      "690 of 1000 have been requested\n",
      "700 of 1000 have been requested\n",
      "710 of 1000 have been requested\n",
      "720 of 1000 have been requested\n",
      "730 of 1000 have been requested\n",
      "740 of 1000 have been requested\n",
      "750 of 1000 have been requested\n",
      "760 of 1000 have been requested\n",
      "770 of 1000 have been requested\n",
      "780 of 1000 have been requested\n",
      "790 of 1000 have been requested\n",
      "800 of 1000 have been requested\n",
      "810 of 1000 have been requested\n",
      "820 of 1000 have been requested\n",
      "830 of 1000 have been requested\n",
      "840 of 1000 have been requested\n",
      "850 of 1000 have been requested\n",
      "860 of 1000 have been requested\n",
      "870 of 1000 have been requested\n",
      "880 of 1000 have been requested\n",
      "890 of 1000 have been requested\n",
      "900 of 1000 have been requested\n",
      "910 of 1000 have been requested\n",
      "920 of 1000 have been requested\n",
      "930 of 1000 have been requested\n",
      "940 of 1000 have been requested\n",
      "950 of 1000 have been requested\n",
      "960 of 1000 have been requested\n",
      "970 of 1000 have been requested\n",
      "980 of 1000 have been requested\n",
      "990 of 1000 have been requested\n",
      "1000 articles' IDs have been stored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 articles are done\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aadit\\Life Buckets\\Life in America\\Academics Related\\Quarter 4\\HCDE\\Homeworks\\hw2\\data-512-homework_2\\data_processing.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aadit/Life%20Buckets/Life%20in%20America/Academics%20Related/Quarter%204/HCDE/Homeworks/hw2/data-512-homework_2/data_processing.ipynb#Y132sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m batch_result \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aadit/Life%20Buckets/Life%20in%20America/Academics%20Related/Quarter%204/HCDE/Homeworks/hw2/data-512-homework_2/data_processing.ipynb#Y132sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbatch_number\u001b[39m\u001b[39m\"\u001b[39m: batch_number,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aadit/Life%20Buckets/Life%20in%20America/Academics%20Related/Quarter%204/HCDE/Homeworks/hw2/data-512-homework_2/data_processing.ipynb#Y132sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpage_info_dict\u001b[39m\u001b[39m\"\u001b[39m: page_info_dict,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aadit/Life%20Buckets/Life%20in%20America/Academics%20Related/Quarter%204/HCDE/Homeworks/hw2/data-512-homework_2/data_processing.ipynb#Y132sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39munable_to_get\u001b[39m\u001b[39m\"\u001b[39m: unable_to_get\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aadit/Life%20Buckets/Life%20in%20America/Academics%20Related/Quarter%204/HCDE/Homeworks/hw2/data-512-homework_2/data_processing.ipynb#Y132sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aadit/Life%20Buckets/Life%20in%20America/Academics%20Related/Quarter%204/HCDE/Homeworks/hw2/data-512-homework_2/data_processing.ipynb#Y132sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m results\u001b[39m.\u001b[39mappend(batch_result)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aadit/Life%20Buckets/Life%20in%20America/Academics%20Related/Quarter%204/HCDE/Homeworks/hw2/data-512-homework_2/data_processing.ipynb#Y132sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39msleep(\u001b[39m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aadit\\anaconda3\\envs\\hcde\\Lib\\asyncio\\tasks.py:639\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(delay, result)\u001b[0m\n\u001b[0;32m    635\u001b[0m h \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39mcall_later(delay,\n\u001b[0;32m    636\u001b[0m                     futures\u001b[39m.\u001b[39m_set_result_unless_cancelled,\n\u001b[0;32m    637\u001b[0m                     future, result)\n\u001b[0;32m    638\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m future\n\u001b[0;32m    640\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    641\u001b[0m     h\u001b[39m.\u001b[39mcancel()\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 1000\n",
    "batch_number = 0\n",
    "\n",
    "# Create a list to store the results of each batch\n",
    "results = []\n",
    "\n",
    "# Iterate over the article_list in batches of size batch_size\n",
    "for i in range(0, len(article_list), batch_size):\n",
    "    batch_number += 1\n",
    "    article_list1 = article_list[i:i + batch_size]\n",
    "    page_info_dict = {}\n",
    "    unable_to_get = {}\n",
    "    await main(article_list1, page_info_dict, unable_to_get)\n",
    "    \n",
    "    # Save the results of this batch to a JSON file\n",
    "    batch_result = {\n",
    "        \"batch_number\": batch_number,\n",
    "        \"page_info_dict\": page_info_dict,\n",
    "        \"unable_to_get\": unable_to_get\n",
    "    }\n",
    "    results.append(batch_result)\n",
    "\n",
    "    await asyncio.sleep(10) #to reduce the burden on the servers and get less failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 2:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 3:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 4:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 5:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 6:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 7:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 8:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 9:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 10:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 11:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 12:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 13:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 14:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 15:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 16:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 17:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 18:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 19:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 20:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 21:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 22:\n",
      "page_info_dict length: 515\n",
      "unable_to_get length: 0\n"
     ]
    }
   ],
   "source": [
    "#Check for any values that were not retreived\n",
    "for batch_result in results:\n",
    "    print(f\"Batch {batch_result['batch_number']}:\")\n",
    "    print(f\"page_info_dict length: {len(batch_result['page_info_dict'])}\")\n",
    "    print(f\"unable_to_get length: {len(batch_result['unable_to_get'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all page_info_dict dictionaries\n",
    "all_page_info_dict = {}\n",
    "for batch_result in results:\n",
    "    all_page_info_dict.update(batch_result['page_info_dict'])\n",
    "\n",
    "print(len(all_page_info_dict))\n",
    "\n",
    "with open('data/all_page_info_dict.json', 'w') as json_file:\n",
    "    json.dump(all_page_info_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_id = pd.DataFrame(list(all_page_info_dict.items()), columns=['page_title', 'rev_id'])\n",
    "us_cities_by_state = us_cities_by_state.merge(rev_id,on='page_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state         0\n",
       "page_title    0\n",
       "url           0\n",
       "rev_id        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_by_state.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_by_state.to_csv('data/cleaned_us_cities_by_state_with_id.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no empty values, so it seems to be good for further processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Getting the ORES scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_by_state = pd.read_csv('data/cleaned_us_cities_by_state_with_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, urllib.parse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tracemalloc\n",
    "# tracemalloc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"aadi2000@uw.edu\"\n",
    "username = \"DrSniperwolf\"\n",
    "access_token = \"<ENTER YOUR ACCESS TOKEN HERE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using async functions to generate multiple requests\n",
    "import asyncio\n",
    "import json\n",
    "import requests\n",
    "import aiohttp\n",
    "\n",
    "\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "API_LATENCY_ASSUMED = 0.002\n",
    "API_THROTTLE_WAIT = (60.0 / 5000.0) - API_LATENCY_ASSUMED\n",
    "\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<\"+email+\">, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer \"+access_token\n",
    "}\n",
    "\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address': email,\n",
    "    'access_token': access_token\n",
    "}\n",
    "\n",
    "ex_article_revisions = {'Bison': 1085687913, 'Northern flicker': 1086582504, 'Red squirrel': 1083787665,\n",
    "                        'Chinook salmon': 1085406228, 'Horseshoe bat': 1060601936}\n",
    "\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\": \"en\",\n",
    "    \"rev_id\": \"\",\n",
    "    \"features\": True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "article_list = list(us_cities_by_state.page_title)\n",
    "article_list = article_list[21500:25000]\n",
    "\n",
    "ores_scores = {}\n",
    "no_prediction = []\n",
    "key_error_list = []\n",
    "\n",
    "article_id = {}\n",
    "\n",
    "with open('data/all_page_info_dict.json', 'r') as file:\n",
    "    article_id = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import aiohttp\n",
    "\n",
    "# ... Your existing constants and data ...\n",
    "\n",
    "async def request_ores_score_per_article(session, article_revid=None, email_address=None, access_token=None,\n",
    "                                        endpoint_url=API_ORES_LIFTWING_ENDPOINT,\n",
    "                                        model_name=API_ORES_EN_QUALITY_MODEL,\n",
    "                                        request_data=ORES_REQUEST_DATA_TEMPLATE,\n",
    "                                        header_format=REQUEST_HEADER_TEMPLATE,\n",
    "                                        header_params=REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "\n",
    "    headers = {}\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "\n",
    "    try:\n",
    "        print(request_data)\n",
    "        async with session.post(request_url, headers=headers, data=json.dumps(request_data)) as response:\n",
    "            json_response = await response.json()\n",
    "        print(f\"Task completed for rev_id {article_revid}\")  # Add this line for debugging\n",
    "        #print(json_response)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "async def process_articles():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        article_count = 0\n",
    "        tasks = []\n",
    "        articles_processed = 0  # Counter for articles processed\n",
    "        articles_per_iteration = 1000 # Number of articles to process in each iteration\n",
    "\n",
    "        for article in article_list:\n",
    "            task = request_ores_score_per_article(session, article_revid=article_id[article],\n",
    "                                                  email_address=email, access_token=access_token)\n",
    "            tasks.append(task)\n",
    "\n",
    "            # Increment the counter\n",
    "            articles_processed += 1\n",
    "\n",
    "            if articles_processed % articles_per_iteration == 0:\n",
    "                # If 1000 articles have been processed, gather the results\n",
    "                results = await asyncio.gather(*tasks)\n",
    "                for article, score in zip(article_list[articles_processed - articles_per_iteration:articles_processed], results):\n",
    "                    if score is None:\n",
    "                        no_prediction.append(article)\n",
    "                    else:\n",
    "                        try:\n",
    "                            print(article, score)\n",
    "                            ores_scores[article] = score['enwiki']['scores'][str(article_id[article])]['articlequality']['score']['prediction']\n",
    "                        except KeyError:\n",
    "                            key_error_list.append(article)\n",
    "\n",
    "                # Reset tasks for the next iteration\n",
    "                tasks = []\n",
    "\n",
    "                # Wait for 5 seconds\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Save the final output to JSON\n",
    "        ores_scores_json_object = json.dumps(ores_scores, indent=4)\n",
    "        with open(f'ores_scores_final_6.json', 'w') as outfile:\n",
    "            outfile.write(ores_scores_json_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_21684\\4202404413.py:1: RuntimeWarning: coroutine 'request_ores_score_per_article' was never awaited\n",
      "  await process_articles()\n",
      "Object allocated at (most recent call last):\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_21684\\958723189.py\", lineno 11\n",
      "    task = request_ores_score_per_article(session, article_revid=article_id[article],\n"
     ]
    }
   ],
   "source": [
    "await process_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Merging all the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to merge merged_data_ores.json and the us_cities_by_state-cleaned to a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/merged_data_ores.json\", 'r') as file:\n",
    "    ores_final = json.load(file)\n",
    "\n",
    "us_cities_by_state = pd.read_csv('data/cleaned_us_cities_by_state_with_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21515 21515\n"
     ]
    }
   ],
   "source": [
    "print(us_cities_by_state.shape[0],len(ores_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we have captured the values for all the articles. Let us join these 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on 'page_title'\n",
    "us_cities_by_state['quality'] = us_cities_by_state['page_title'].map(ores_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21510</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wamsutter,_Wyoming</td>\n",
       "      <td>1169591845</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21511</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wheatland,_Wyoming</td>\n",
       "      <td>1176370621</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21512</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Worland,_Wyoming</td>\n",
       "      <td>1166347917</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21513</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wright,_Wyoming</td>\n",
       "      <td>1166334449</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21514</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Yoder,_Wyoming</td>\n",
       "      <td>1171182284</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21515 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state           page_title  \\\n",
       "0      Alabama   Abbeville, Alabama   \n",
       "1      Alabama  Adamsville, Alabama   \n",
       "2      Alabama     Addison, Alabama   \n",
       "3      Alabama       Akron, Alabama   \n",
       "4      Alabama   Alabaster, Alabama   \n",
       "...        ...                  ...   \n",
       "21510  Wyoming   Wamsutter, Wyoming   \n",
       "21511  Wyoming   Wheatland, Wyoming   \n",
       "21512  Wyoming     Worland, Wyoming   \n",
       "21513  Wyoming      Wright, Wyoming   \n",
       "21514  Wyoming       Yoder, Wyoming   \n",
       "\n",
       "                                                     url      rev_id quality  \n",
       "0       https://en.wikipedia.org/wiki/Abbeville,_Alabama  1171163550       C  \n",
       "1      https://en.wikipedia.org/wiki/Adamsville,_Alabama  1177621427       C  \n",
       "2         https://en.wikipedia.org/wiki/Addison,_Alabama  1168359898       C  \n",
       "3           https://en.wikipedia.org/wiki/Akron,_Alabama  1165909508      GA  \n",
       "4       https://en.wikipedia.org/wiki/Alabaster,_Alabama  1179139816       C  \n",
       "...                                                  ...         ...     ...  \n",
       "21510   https://en.wikipedia.org/wiki/Wamsutter,_Wyoming  1169591845      GA  \n",
       "21511   https://en.wikipedia.org/wiki/Wheatland,_Wyoming  1176370621      GA  \n",
       "21512     https://en.wikipedia.org/wiki/Worland,_Wyoming  1166347917      GA  \n",
       "21513      https://en.wikipedia.org/wiki/Wright,_Wyoming  1166334449      GA  \n",
       "21514       https://en.wikipedia.org/wiki/Yoder,_Wyoming  1171182284       C  \n",
       "\n",
       "[21515 rows x 5 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [state, page_title, url, rev_id, quality]\n",
       "Index: []"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_by_state[us_cities_by_state['quality'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now add the population data and the regions information for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the US population data from an Excel file, skipping the first 4 rows of metadata.\n",
    "us_population_data = pd.read_excel('data/NST-EST2022-POP.xlsx', skiprows=4)\n",
    "\n",
    "# Remove unnecessary rows and reset the index.\n",
    "us_population_data = us_population_data[4:]\n",
    "us_population_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename the columns for clarity.\n",
    "us_population_data.columns = ['state', '2020_est', '2020', '2021', '2022']\n",
    "\n",
    "# Filter and clean the data: Keep only rows where the 'state' names start with a period.\n",
    "us_population_data = us_population_data[us_population_data['state'].str.contains('^\\.', na=False)]\n",
    "us_population_data['state'] = us_population_data['state'].str.slice(1)\n",
    "us_population_data = us_population_data[['state', '2022']].reset_index(drop=True)\n",
    "\n",
    "# Rename the columns to reflect the year.\n",
    "us_population_data.columns = ['state', 'Population_2022']\n",
    "\n",
    "#us_population_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regional division data from an Excel file\n",
    "us_regions = pd.read_excel('data/US States by Region - US Census Bureau.xlsx')\n",
    "\n",
    "# Fill missing REGION and DIVISION values with the previous valid value\n",
    "us_regions['REGION'].ffill(inplace=True)\n",
    "us_regions['DIVISION'].ffill(inplace=True)\n",
    "\n",
    "# Filter rows with valid STATE entries (remove non-state rows)\n",
    "us_regions = us_regions.dropna(subset=['STATE'])\n",
    "\n",
    "# Convert column names to lowercase for consistency\n",
    "us_regions.columns = us_regions.columns.str.lower()\n",
    "\n",
    "#us_regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Delaware', 'Florida', 'Georgia_(U.S._state)',\n",
       "       'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas',\n",
       "       'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts',\n",
       "       'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana',\n",
       "       'Nevada', 'New_Hampshire', 'New_Jersey', 'New_Mexico', 'New_York',\n",
       "       'North_Carolina', 'North_Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
       "       'Pennsylvania', 'Rhode_Island', 'South_Carolina', 'South_Dakota',\n",
       "       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West_Virginia', 'Wisconsin', 'Wyoming'], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_by_state['state'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we need to clean some of the state names to merge with the other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean state names, handling any specific cases\n",
    "us_cities_by_state['state'] = us_cities_by_state['state'].apply(lambda x: 'Georgia' if x == 'Georgia_(U.S._state)' else x)\n",
    "\n",
    "# The cityxstate data had values like New_york, fixing that\n",
    "us_cities_by_state['state'] = us_cities_by_state.state.str.replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21515, 8)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge all the datasets\n",
    "temp1 = us_regions.merge(us_population_data, on = 'state').merge(us_cities_by_state, on='state')\n",
    "temp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.drop(columns = ['url','region'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "renaming = {\n",
    "    'division': 'regional_division',\n",
    "    'Population_2022': 'population',\n",
    "    'page_title':'article_title',\n",
    "    'rev_id': 'revision_id',\n",
    "    'quality':'article_quality'\n",
    "}\n",
    "\n",
    "temp1.rename(columns=renaming, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regional_division</th>\n",
       "      <th>state</th>\n",
       "      <th>population</th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20851</th>\n",
       "      <td>Pacific</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1440196.0</td>\n",
       "      <td>'Ewa Gentry, Hawaii</td>\n",
       "      <td>419689771</td>\n",
       "      <td>Stub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15396</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5074296.0</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14015</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>10912876.0</td>\n",
       "      <td>Abbeville, Georgia</td>\n",
       "      <td>1171167087</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17424</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>4590241.0</td>\n",
       "      <td>Abbeville, Louisiana</td>\n",
       "      <td>1178840199</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16278</th>\n",
       "      <td>East South Central</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>2940057.0</td>\n",
       "      <td>Abbeville, Mississippi</td>\n",
       "      <td>1171172603</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14014</th>\n",
       "      <td>South Atlantic</td>\n",
       "      <td>Florida</td>\n",
       "      <td>22244823.0</td>\n",
       "      <td>Zolfo Springs, Florida</td>\n",
       "      <td>1171166674</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11771</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>5717184.0</td>\n",
       "      <td>Zumbro Falls, Minnesota</td>\n",
       "      <td>1165908705</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11252</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>5717184.0</td>\n",
       "      <td>Zumbrota, Minnesota</td>\n",
       "      <td>1170033989</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11011</th>\n",
       "      <td>West North Central</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>3200517.0</td>\n",
       "      <td>Zwingle, Iowa</td>\n",
       "      <td>1171169323</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17727</th>\n",
       "      <td>West South Central</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>4590241.0</td>\n",
       "      <td>Zwolle, Louisiana</td>\n",
       "      <td>1180123879</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21515 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        regional_division        state  population            article_title  \\\n",
       "20851             Pacific       Hawaii   1440196.0      'Ewa Gentry, Hawaii   \n",
       "15396  East South Central      Alabama   5074296.0       Abbeville, Alabama   \n",
       "14015      South Atlantic      Georgia  10912876.0       Abbeville, Georgia   \n",
       "17424  West South Central    Louisiana   4590241.0     Abbeville, Louisiana   \n",
       "16278  East South Central  Mississippi   2940057.0   Abbeville, Mississippi   \n",
       "...                   ...          ...         ...                      ...   \n",
       "14014      South Atlantic      Florida  22244823.0   Zolfo Springs, Florida   \n",
       "11771  West North Central    Minnesota   5717184.0  Zumbro Falls, Minnesota   \n",
       "11252  West North Central    Minnesota   5717184.0      Zumbrota, Minnesota   \n",
       "11011  West North Central         Iowa   3200517.0            Zwingle, Iowa   \n",
       "17727  West South Central    Louisiana   4590241.0        Zwolle, Louisiana   \n",
       "\n",
       "       revision_id article_quality  \n",
       "20851    419689771            Stub  \n",
       "15396   1171163550               C  \n",
       "14015   1171167087               C  \n",
       "17424   1178840199               C  \n",
       "16278   1171172603               C  \n",
       "...            ...             ...  \n",
       "14014   1171166674              GA  \n",
       "11771   1165908705               C  \n",
       "11252   1170033989               C  \n",
       "11011   1171169323               C  \n",
       "17727   1180123879               C  \n",
       "\n",
       "[21515 rows x 6 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1.sort_values('article_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.to_csv('data/wp_scored_city_articles_by_state.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
