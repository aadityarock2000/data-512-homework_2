{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - Considering Bias in Data\n",
    "\n",
    "## Pre Processing for Analysis\n",
    "\n",
    "## License\n",
    "This code example was developed by Aaditya Parthasarathy for a homework assignment in DATA 512, a course in the UW MS Data Science degree program. Some of the code is partially adopted from the jupyter notebook developed by Dr. David W. McDonald, which is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.1 - August 14, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Getting the data and initial cleanup\n",
    "\n",
    "We take our first step in importing the articles and cleaning it up by removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "import aiohttp\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the article list, which is in the data folder as us_cities_by_state_SEPT.2023.csv for cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the article list with states and cities\n",
    "us_cities_by_state = pd.read_csv('data/us_cities_by_state_SEPT.2023.csv')\n",
    "\n",
    "# Dropping duplicates due to inconsistencies with the state list\n",
    "us_cities_by_state.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>2020 United States census</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2020_United_Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>2010 United States census</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2010_United_Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>Florida</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12947</th>\n",
       "      <td>New_York</td>\n",
       "      <td>Population</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18433</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18780</th>\n",
       "      <td>Texas</td>\n",
       "      <td>Population</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18781</th>\n",
       "      <td>Texas</td>\n",
       "      <td>2020 United States census</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2020_United_Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18782</th>\n",
       "      <td>Texas</td>\n",
       "      <td>2010 United States census</td>\n",
       "      <td>https://en.wikipedia.org/wiki/2010_United_Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21234</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>County (United States)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/County_(United_S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state                 page_title  \\\n",
       "1683    Colorado  2020 United States census   \n",
       "1684    Colorado  2010 United States census   \n",
       "2030     Florida     County (United States)   \n",
       "5196        Iowa     County (United States)   \n",
       "12947   New_York                 Population   \n",
       "18433  Tennessee     County (United States)   \n",
       "18780      Texas                 Population   \n",
       "18781      Texas  2020 United States census   \n",
       "18782      Texas  2010 United States census   \n",
       "21234  Wisconsin     County (United States)   \n",
       "\n",
       "                                                     url  \n",
       "1683   https://en.wikipedia.org/wiki/2020_United_Stat...  \n",
       "1684   https://en.wikipedia.org/wiki/2010_United_Stat...  \n",
       "2030   https://en.wikipedia.org/wiki/County_(United_S...  \n",
       "5196   https://en.wikipedia.org/wiki/County_(United_S...  \n",
       "12947           https://en.wikipedia.org/wiki/Population  \n",
       "18433  https://en.wikipedia.org/wiki/County_(United_S...  \n",
       "18780           https://en.wikipedia.org/wiki/Population  \n",
       "18781  https://en.wikipedia.org/wiki/2020_United_Stat...  \n",
       "18782  https://en.wikipedia.org/wiki/2010_United_Stat...  \n",
       "21234  https://en.wikipedia.org/wiki/County_(United_S...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to see if we missed any unwanted entries\n",
    "a = list(us_cities_by_state.page_title)\n",
    "b = {}\n",
    "c = set()\n",
    "\n",
    "for state in a:\n",
    "    if state in b:\n",
    "        c.add(state)\n",
    "    else:\n",
    "        b[state] = 1\n",
    "us_cities_by_state[us_cities_by_state['page_title'].isin(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to remove these rows from the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_by_state.drop(us_cities_by_state[us_cities_by_state['page_title'].isin(c)].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21520</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wamsutter,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21521</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wheatland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21522</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Worland,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21523</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wright,_Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21524</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Yoder,_Wyoming</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21515 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state           page_title  \\\n",
       "0      Alabama   Abbeville, Alabama   \n",
       "1      Alabama  Adamsville, Alabama   \n",
       "2      Alabama     Addison, Alabama   \n",
       "3      Alabama       Akron, Alabama   \n",
       "4      Alabama   Alabaster, Alabama   \n",
       "...        ...                  ...   \n",
       "21520  Wyoming   Wamsutter, Wyoming   \n",
       "21521  Wyoming   Wheatland, Wyoming   \n",
       "21522  Wyoming     Worland, Wyoming   \n",
       "21523  Wyoming      Wright, Wyoming   \n",
       "21524  Wyoming       Yoder, Wyoming   \n",
       "\n",
       "                                                     url  \n",
       "0       https://en.wikipedia.org/wiki/Abbeville,_Alabama  \n",
       "1      https://en.wikipedia.org/wiki/Adamsville,_Alabama  \n",
       "2         https://en.wikipedia.org/wiki/Addison,_Alabama  \n",
       "3           https://en.wikipedia.org/wiki/Akron,_Alabama  \n",
       "4       https://en.wikipedia.org/wiki/Alabaster,_Alabama  \n",
       "...                                                  ...  \n",
       "21520   https://en.wikipedia.org/wiki/Wamsutter,_Wyoming  \n",
       "21521   https://en.wikipedia.org/wiki/Wheatland,_Wyoming  \n",
       "21522     https://en.wikipedia.org/wiki/Worland,_Wyoming  \n",
       "21523      https://en.wikipedia.org/wiki/Wright,_Wyoming  \n",
       "21524       https://en.wikipedia.org/wiki/Yoder,_Wyoming  \n",
       "\n",
       "[21515 rows x 3 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_by_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now ready for processing. We can note here that we have 21515 articles/places for which we fetch the information of last revision id and the quality scores. This would be a helpful number to check and keep track of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Getting last revision id for an article\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is based on a Jupyter notebook originally developed by Dr. David W. McDonald. I have adapted it to use asynchronous functions, allowing for concurrent requests and faster processing. It's important to be aware that frequent or intensive use of this code may lead to temporary or permanent API access restrictions, so exercise caution. You can view the original code in helper_code/wp_page_info_example.ipynb\n",
    "\n",
    "Additionally, please remember to configure the REQUEST_HEADERS variable with your email address. Be sure to review the API documentation for information on rate limits and adjust this code as needed to suit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Define constants\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_THROTTLE_WAIT = 0.1\n",
    "\n",
    "# Define other constants and variables\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<aadi2000@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",  # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n",
    "       \n",
    "# this is an async function to create and pull multiple requests from the API.\n",
    "async def request_pageinfo_per_article(session, article_title, endpoint_url=API_ENWIKIPEDIA_ENDPOINT,\n",
    "                                       request_template=PAGEINFO_PARAMS_TEMPLATE, headers=REQUEST_HEADERS):\n",
    "    request_template['titles'] = article_title\n",
    "\n",
    "    if not request_template['titles']:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "\n",
    "    async with session.get(endpoint_url, headers=headers, params=request_template) as response:\n",
    "        try:\n",
    "            json_response = await response.json()\n",
    "            # Get the page ID for the article\n",
    "            page_id = list(json_response['query']['pages'].keys())[0]\n",
    "            lastrevid = json_response['query']['pages'][page_id]['lastrevid']\n",
    "            return lastrevid\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to retrieve data for {article_title}: {e}\")\n",
    "            return None\n",
    "\n",
    "# This is the main function, where we create multiple sessions, and fetch the last revision id\n",
    "async def main(article_titles, page_info_dict, unable_to_get):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        count = 0  # Initialize a counter\n",
    "        for i, title in enumerate(article_titles):\n",
    "            task = request_pageinfo_per_article(session, title)\n",
    "            tasks.append(task)\n",
    "            if i % 10 == 0:\n",
    "                print(f\"{i} of {len(article_titles)} have been requested\")\n",
    "                \n",
    "            if (i + 1) % 1000 == 0:\n",
    "                print(f\"{i + 1} articles' IDs have been stored\")\n",
    "                count += 1\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "        #once we get the scores, we can look at and store the ids in a dictionary\n",
    "        for i, response in enumerate(responses):\n",
    "            if response is not None:\n",
    "                page_info_dict[article_titles[i]] = response\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data for {article_titles[i]}\")\n",
    "                unable_to_get[article_titles[i]] = 1\n",
    "        print(f\"{count * 1000} articles are done\")\n",
    "    return page_info_dict, unable_to_get\n",
    "\n",
    "# Define the list of articles\n",
    "article_list = list(us_cities_by_state[\"page_title\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we perform it asycronously, there are chances of it stopping in the middle. to save the processed information, we take a batch size of 1000, and save for every 1000 articles performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 1000\n",
    "batch_number = 0\n",
    "\n",
    "# Create a list to store the results of each batch\n",
    "results = []\n",
    "\n",
    "# Iterate over the article_list in batches of size batch_size\n",
    "for i in range(0, len(article_list), batch_size):\n",
    "    batch_number += 1\n",
    "    article_list1 = article_list[i:i + batch_size]\n",
    "    page_info_dict = {}\n",
    "    unable_to_get = {}\n",
    "    await main(article_list1, page_info_dict, unable_to_get)\n",
    "    \n",
    "    # Save the results of this batch to a JSON file\n",
    "    batch_result = {\n",
    "        \"batch_number\": batch_number,\n",
    "        \"page_info_dict\": page_info_dict,\n",
    "        \"unable_to_get\": unable_to_get\n",
    "    }\n",
    "    results.append(batch_result)\n",
    "\n",
    "    await asyncio.sleep(10) #to reduce the burden on the servers and get less failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets see if me missed any articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 2:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 3:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 4:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 5:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 6:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 7:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 8:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 9:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 10:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 11:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 12:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 13:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 14:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 15:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 16:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 17:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 18:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 19:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 20:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 21:\n",
      "page_info_dict length: 1000\n",
      "unable_to_get length: 0\n",
      "Batch 22:\n",
      "page_info_dict length: 515\n",
      "unable_to_get length: 0\n"
     ]
    }
   ],
   "source": [
    "#Check for any values that were not retreived\n",
    "for batch_result in results:\n",
    "    print(f\"Batch {batch_result['batch_number']}:\")\n",
    "    print(f\"page_info_dict length: {len(batch_result['page_info_dict'])}\")\n",
    "    print(f\"unable_to_get length: {len(batch_result['unable_to_get'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the code worked as intended and processed all the articles. Let us store this to prevent any loss of data and avoiding rerunning of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all page_info_dict dictionaries\n",
    "all_page_info_dict = {}\n",
    "for batch_result in results:\n",
    "    all_page_info_dict.update(batch_result['page_info_dict'])\n",
    "\n",
    "print(len(all_page_info_dict))\n",
    "\n",
    "with open('data/all_page_info_dict.json', 'w') as json_file:\n",
    "    json.dump(all_page_info_dict, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now merge this to the us_cities_by_state.\n",
    "rev_id = pd.DataFrame(list(all_page_info_dict.items()), columns=['page_title', 'rev_id'])\n",
    "us_cities_by_state = us_cities_by_state.merge(rev_id,on='page_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state         0\n",
       "page_title    0\n",
       "url           0\n",
       "rev_id        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for nulls\n",
    "us_cities_by_state.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully obtained the last revision id for all the articles. Let us save this dataframe to avoid running this code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_by_state.to_csv('data/cleaned_us_cities_by_state_with_id.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no empty values, so it seems to be good for further processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Getting the ORES scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities_by_state = pd.read_csv('data/cleaned_us_cities_by_state_with_id.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now work on obtaining the article quality scores for each article using the ORES_LEFTWING MODEL. You can read more about this here: [ORES LiftWing documentation](https://wikitech.wikimedia.org/wiki/Machine_Learning/LiftWing/Usage)\n",
    "\n",
    "The following code example was heavily inspired from the code developed by Dr. David W. McDonald for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [Creative Commons](https://creativecommons.org) [CC-BY license](https://creativecommons.org/licenses/by/4.0/). Revision 1.0 - August 15, 2023. You can look at this code in the helper_code/wp_ores_leftwing_example.ipynb. Also, please refer to this documentation to create API keys and check them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTANTS - API REQUIREMENTS\n",
    "email = \"<ENTER YOUR EMAIL HERE>\"\n",
    "access_token = \"<ENTER YOUR ACCESS TOKEN HERE>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using async functions to generate multiple requests\n",
    "\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/{model_name}:predict\"\n",
    "API_ORES_EN_QUALITY_MODEL = \"enwiki-articlequality\"\n",
    "API_LATENCY_ASSUMED = 0.002\n",
    "API_THROTTLE_WAIT = (60.0 / 5000.0) - API_LATENCY_ASSUMED\n",
    "\n",
    "REQUEST_HEADER_TEMPLATE = {\n",
    "    'User-Agent': \"<\"+email+\">, University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer \"+access_token\n",
    "}\n",
    "\n",
    "REQUEST_HEADER_PARAMS_TEMPLATE = {\n",
    "    'email_address': email,\n",
    "    'access_token': access_token\n",
    "}\n",
    "\n",
    "\n",
    "ORES_REQUEST_DATA_TEMPLATE = {\n",
    "    \"lang\": \"en\",\n",
    "    \"rev_id\": \"\",\n",
    "    \"features\": True\n",
    "}\n",
    "\n",
    "\n",
    "ores_scores = {}\n",
    "no_prediction = []\n",
    "key_error_list = []\n",
    "\n",
    "article_id = {}\n",
    "\n",
    "with open('data/all_page_info_dict.json', 'r') as file:\n",
    "    article_id = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is also modified to work asyncronously like the previous one. However, note that we are rate limited to only 5000 requests per hour on the personal API option, here I have sliced the articles into batches of 5000 to make this run faster, and rerun this every hour or so using updated slicers to avoind getting the HTTP code:429 for rate limit exceeded. \n",
    "\n",
    "Please exercise caution while using the following code, as this might immediately exhause your quota for the hour. Repeated use of this code may lead to API restrictions/ban if misused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of articles to be processed.\n",
    "article_list = list(us_cities_by_state.page_title)\n",
    "article_list = article_list[21500:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified request_ores_score_per_article from helper_code/wp_ores_leftwing_example.ipynb to work async\n",
    "async def request_ores_score_per_article(session, article_revid=None, email_address=None, access_token=None,\n",
    "                                        endpoint_url=API_ORES_LIFTWING_ENDPOINT,\n",
    "                                        model_name=API_ORES_EN_QUALITY_MODEL,\n",
    "                                        request_data=ORES_REQUEST_DATA_TEMPLATE,\n",
    "                                        header_format=REQUEST_HEADER_TEMPLATE,\n",
    "                                        header_params=REQUEST_HEADER_PARAMS_TEMPLATE):\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    if email_address:\n",
    "        header_params['email_address'] = email_address\n",
    "    if access_token:\n",
    "        header_params['access_token'] = access_token\n",
    "\n",
    "    if not request_data['rev_id']:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    if not header_params['email_address']:\n",
    "        raise Exception(\"Must provide an 'email_address' value\")\n",
    "    if not header_params['access_token']:\n",
    "        raise Exception(\"Must provide an 'access_token' value\")\n",
    "\n",
    "    request_url = endpoint_url.format(model_name=model_name)\n",
    "\n",
    "    headers = {}\n",
    "    for key in header_format.keys():\n",
    "        headers[str(key)] = header_format[key].format(**header_params)\n",
    "\n",
    "    try:\n",
    "        print(request_data)\n",
    "        async with session.post(request_url, headers=headers, data=json.dumps(request_data)) as response:\n",
    "            json_response = await response.json()\n",
    "        print(f\"Task completed for rev_id {article_revid}\")  # Add this line for debugging\n",
    "        #print(json_response)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used a iteration of 1000, where the process_articles function saves the data into a json file for every 1000 successful iterations. You can modify this as you see fit, but this worked for my present data with no hiccups for 5000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_articles():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        article_count = 0\n",
    "        tasks = []\n",
    "        articles_processed = 0  # Counter for articles processed\n",
    "        articles_per_iteration = 1000 # Number of articles to process in each iteration\n",
    "\n",
    "        for article in article_list:\n",
    "            task = request_ores_score_per_article(session, article_revid=article_id[article],\n",
    "                                                  email_address=email, access_token=access_token)\n",
    "            tasks.append(task)\n",
    "\n",
    "            # Increment the counter\n",
    "            articles_processed += 1\n",
    "\n",
    "            if articles_processed % articles_per_iteration == 0:\n",
    "                # If 1000 articles have been processed, gather the results\n",
    "                results = await asyncio.gather(*tasks)\n",
    "                for article, score in zip(article_list[articles_processed - articles_per_iteration:articles_processed], results):\n",
    "                    if score is None:\n",
    "                        no_prediction.append(article)\n",
    "                    else:\n",
    "                        try:\n",
    "                            print(article, score)\n",
    "                            ores_scores[article] = score['enwiki']['scores'][str(article_id[article])]['articlequality']['score']['prediction']\n",
    "                        except KeyError:\n",
    "                            key_error_list.append(article)\n",
    "\n",
    "                # Reset tasks for the next iteration\n",
    "                tasks = []\n",
    "\n",
    "                # Wait for 5 seconds\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Save the final output to JSON\n",
    "        ores_scores_json_object = json.dumps(ores_scores, indent=4)\n",
    "        with open(f'ores_scores_final_6.json', 'w') as outfile:\n",
    "            outfile.write(ores_scores_json_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this code to process for the sliced 5000 articles that are sliced\n",
    "await process_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully saved all the requested articles, and there seems to be no articles missed, which is great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Merging all the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge all the quality scores from the data/ores folder to a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "directory_path = \"data/ores_files\"\n",
    "\n",
    "# Initialize an empty dictionary for merging\n",
    "merged_data = {}\n",
    "\n",
    "# List JSON files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Load the contents of the JSON file\n",
    "        with open(file_path, \"r\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "        # Update the merged dictionary with the data\n",
    "        merged_data.update(data)\n",
    "\n",
    "# Save the merged data to a new JSON file\n",
    "output_file_path = \"data/merged_data.json\"\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    json.dump(merged_data, output_file, indent=4)\n",
    "\n",
    "print(f\"Merged data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to merge merged_data_ores.json and the us_cities_by_state-cleaned to a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/merged_data_ores.json\", 'r') as file:\n",
    "    ores_final = json.load(file)\n",
    "\n",
    "us_cities_by_state = pd.read_csv('data/cleaned_us_cities_by_state_with_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21515 21515\n"
     ]
    }
   ],
   "source": [
    "print(us_cities_by_state.shape[0],len(ores_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we have captured the values for all the articles. Let us join these 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on 'page_title'\n",
    "us_cities_by_state['quality'] = us_cities_by_state['page_title'].map(ores_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abbeville,_Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Adamsville,_Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Addison,_Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Akron,_Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alabaster,_Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21510</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wamsutter, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wamsutter,_Wyoming</td>\n",
       "      <td>1169591845</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21511</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wheatland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wheatland,_Wyoming</td>\n",
       "      <td>1176370621</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21512</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Worland, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Worland,_Wyoming</td>\n",
       "      <td>1166347917</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21513</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Wright, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wright,_Wyoming</td>\n",
       "      <td>1166334449</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21514</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Yoder, Wyoming</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Yoder,_Wyoming</td>\n",
       "      <td>1171182284</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21515 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         state           page_title  \\\n",
       "0      Alabama   Abbeville, Alabama   \n",
       "1      Alabama  Adamsville, Alabama   \n",
       "2      Alabama     Addison, Alabama   \n",
       "3      Alabama       Akron, Alabama   \n",
       "4      Alabama   Alabaster, Alabama   \n",
       "...        ...                  ...   \n",
       "21510  Wyoming   Wamsutter, Wyoming   \n",
       "21511  Wyoming   Wheatland, Wyoming   \n",
       "21512  Wyoming     Worland, Wyoming   \n",
       "21513  Wyoming      Wright, Wyoming   \n",
       "21514  Wyoming       Yoder, Wyoming   \n",
       "\n",
       "                                                     url      rev_id quality  \n",
       "0       https://en.wikipedia.org/wiki/Abbeville,_Alabama  1171163550       C  \n",
       "1      https://en.wikipedia.org/wiki/Adamsville,_Alabama  1177621427       C  \n",
       "2         https://en.wikipedia.org/wiki/Addison,_Alabama  1168359898       C  \n",
       "3           https://en.wikipedia.org/wiki/Akron,_Alabama  1165909508      GA  \n",
       "4       https://en.wikipedia.org/wiki/Alabaster,_Alabama  1179139816       C  \n",
       "...                                                  ...         ...     ...  \n",
       "21510   https://en.wikipedia.org/wiki/Wamsutter,_Wyoming  1169591845      GA  \n",
       "21511   https://en.wikipedia.org/wiki/Wheatland,_Wyoming  1176370621      GA  \n",
       "21512     https://en.wikipedia.org/wiki/Worland,_Wyoming  1166347917      GA  \n",
       "21513      https://en.wikipedia.org/wiki/Wright,_Wyoming  1166334449      GA  \n",
       "21514       https://en.wikipedia.org/wiki/Yoder,_Wyoming  1171182284       C  \n",
       "\n",
       "[21515 rows x 5 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_by_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>url</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [state, page_title, url, rev_id, quality]\n",
       "Index: []"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_by_state[us_cities_by_state['quality'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no missing values, which validates our point from above. Let us now add the population data and the regions information for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the US population data from an Excel file, skipping the first 4 rows of metadata.\n",
    "us_population_data = pd.read_excel('data/NST-EST2022-POP.xlsx', skiprows=4)\n",
    "\n",
    "# Remove unnecessary rows and reset the index.\n",
    "us_population_data = us_population_data[4:]\n",
    "us_population_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename the columns for clarity.\n",
    "us_population_data.columns = ['state', '2020_est', '2020', '2021', '2022']\n",
    "\n",
    "# Filter and clean the data: Keep only rows where the 'state' names start with a period.\n",
    "us_population_data = us_population_data[us_population_data['state'].str.contains('^\\.', na=False)]\n",
    "us_population_data['state'] = us_population_data['state'].str.slice(1)\n",
    "us_population_data = us_population_data[['state', '2022']].reset_index(drop=True)\n",
    "\n",
    "# Rename the columns to reflect the year.\n",
    "us_population_data.columns = ['state', 'Population_2022']\n",
    "\n",
    "#us_population_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regional division data from an Excel file\n",
    "us_regions = pd.read_excel('data/US States by Region - US Census Bureau.xlsx')\n",
    "\n",
    "# Fill missing REGION and DIVISION values with the previous valid value\n",
    "us_regions['REGION'].ffill(inplace=True)\n",
    "us_regions['DIVISION'].ffill(inplace=True)\n",
    "\n",
    "# Filter rows with valid STATE entries (remove non-state rows)\n",
    "us_regions = us_regions.dropna(subset=['STATE'])\n",
    "\n",
    "# Convert column names to lowercase for consistency\n",
    "us_regions.columns = us_regions.columns.str.lower()\n",
    "\n",
    "#us_regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Delaware', 'Florida', 'Georgia_(U.S._state)',\n",
       "       'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas',\n",
       "       'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts',\n",
       "       'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana',\n",
       "       'Nevada', 'New_Hampshire', 'New_Jersey', 'New_Mexico', 'New_York',\n",
       "       'North_Carolina', 'North_Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
       "       'Pennsylvania', 'Rhode_Island', 'South_Carolina', 'South_Dakota',\n",
       "       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West_Virginia', 'Wisconsin', 'Wyoming'], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities_by_state['state'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we need to clean some of the state names to merge with the other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean state names, handling any specific cases\n",
    "us_cities_by_state['state'] = us_cities_by_state['state'].apply(lambda x: 'Georgia' if x == 'Georgia_(U.S._state)' else x)\n",
    "\n",
    "# The cityxstate data had values like New_york, fixing that\n",
    "us_cities_by_state['state'] = us_cities_by_state.state.str.replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21515, 8)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge all the datasets\n",
    "temp1 = us_regions.merge(us_population_data, on = 'state').merge(us_cities_by_state, on='state')\n",
    "temp1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataset has taken shape. We can now clean up the names and save this dataframe for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.drop(columns = ['url','region'], inplace = True)\n",
    "renaming = {\n",
    "    'division': 'regional_division',\n",
    "    'Population_2022': 'population',\n",
    "    'page_title':'article_title',\n",
    "    'rev_id': 'revision_id',\n",
    "    'quality':'article_quality'\n",
    "}\n",
    "\n",
    "temp1.rename(columns=renaming, inplace=True)\n",
    "temp1.sort_values('article_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.to_csv('data/wp_scored_city_articles_by_state.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
